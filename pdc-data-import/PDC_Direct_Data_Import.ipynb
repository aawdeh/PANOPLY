{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and processing raw data from the <br>Proteomic Data Commons ([PDC](https://proteomic.datacommons.cancer.gov/pdc/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Overview\n",
    "The **Proteomic Data Commons** ([PDC](https://proteomic.datacommons.cancer.gov/pdc/)) is a data repository within the [NCI Cancer Research Data Commons (CRDC)](https://datacommons.cancer.gov/) and provides access to curated and standardized *proteomic data* along with biospecimen and clinical metadata. The goal of this notebook is to faciliate *automated data download and streamlined analysis* of raw proteomic data imported from the PDC using the [`FragPipe`](https://fragpipe.nesvilab.org/) proteomics pipeline.\n",
    "\n",
    "#### Caveat\n",
    "This notebook is a **proof of principle** and works for *TMT* or *iTRAQ* raw data from the PDC. It may not be robust to different types of inputs and raw data files, and implements only minimal error checking. With knowledge of input requirements for `FragPipe`, this notebook can be modified to support other types of data imports from the PDC.\n",
    "\n",
    "### Using this notebook\n",
    "When data files are selected at the PDC and exported to [Terra](http://app.terra.bio), a `file` table is automatically created and populated in the chosen Terra workspace. The Python code in this notebook transfers data files pointed to by [DRS URIs](https://support.terra.bio/hc/en-us/articles/360039330211-Overview-Interoperable-data-GA4GH-DRS-URIs-) in the `file` table to the Google bucket for the workspace. In addition to transferring raw data files, this script also creates the annotation and manifest files needed to run `FragPipe` on the raw data files using the `panoply_fragpipe_search` workflow from the [PANOPLY](https://github.com/broadinstitute/PANOPLY) proteogenomic data analysis platform.\n",
    "\n",
    "Step-by-step process for importing and processing raw proteomics data files:\n",
    "1. Start by navigating to the [PDC](https://proteomic.datacommons.cancer.gov/pdc/). \n",
    "2. Browse through the proteomics raw data available there and select files to import and process on Terra. Analyzing TMT or iTRAQ raw data using `FragPipe` requires `mzML` files. On the PDC, `mzML` files are listed in the `Processed Mass Spectra (Open Standard)` data category.\n",
    "3. Export file manifest for the chosen files using the `PFB` button. This will connect to Terra and prompt for a workspace to put locate the `file` manifest table.\n",
    "4. Copy the `PDC_Direct_Data_Import` notebook to the workspace with the `file` manifest table and run all the code blocks in sequence. Running the code blocks will:\n",
    "    1. Download all the files listed in the table to the `fragpipe` directory in the workspace bucket.\n",
    "    2. Organize files into subdirectories--one subdirectory for each TMT/iTRAQ plex, including all fractions for that plex.\n",
    "    3. Create annotation files for each TMT/iTRAQ plex to provide sample IDs.\n",
    "    4. Generate a `FragPipe` manifest file for processing all the data files.\n",
    "5. Once the notebook has been successfully run, create a `FragPipe` [workflow](https://fragpipe.nesvilab.org/docs/tutorial_fragpipe_workflows.html) using the `FragPipe` graphical user interface. Workflows can also be directly [downloaded](https://github.com/Nesvilab/FragPipe/tree/master/MSFragger-GUI/workflows) and/or edited as needed. Upload the workflow to the workspace bucket.\n",
    "6. Obtain a relevant (fasta) sequence database and upload to the workspace bucket. Sequence databases can be downloaded using the `FragPipe` graphical user interface, or copied from other workspaces.\n",
    "7. If not already present, import the `panoply_fragpipe_search` method from the [FireCloud Method Repository](https://portal.firecloud.org/?return=terra#methods).\n",
    "8. Set up inputs to the `panoply_fragpipe_search` workflow:\n",
    "    1. Point `database` to the sequence database in the workspace bucket (from Step 6).\n",
    "    2. `files_folder` is the bucket address of the `fragpipe` directory created to host all the downloaded files.\n",
    "    3. The location of the manifest file `fragpipe-manifest.fp-manifest` created by the notebook should be filled into `fragpipe_manifest`.\n",
    "    4. The bucket location of the workflow file uploaded in Step 5 goes in the `fragpipe_workflow` input slot.\n",
    "9. Run the workflow and download the output `zip` files.\n",
    "\n",
    "**NB:** The Python code below is interspersed with comments to describe its function and logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Libraries\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import hashlib\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from terra_notebook_utils import drs, table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set target directory on the Workspace Bucket\n",
    "In the workspace bucket, files are put into the `fragpipe` directory. In this directory, subdirectories are created for each experiment in order to support TMT workflows using FragPipe. All files in the `file` table are transferred. In each subdirectory representing a TMT/iTRAQ plex, an `annotation.txt` file is created to map the TMT/iTRAQ channels to sample or aliquot IDs. Additionally, a `fragpipe-manifest.fp-manifest` manifest file is create in the root directory of the workspace bucket to set up the entire analysis.\n",
    "\n",
    "The code section below establishes various global variables for data transfer, annotation and setup for FragPipe analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     17
    ]
   },
   "outputs": [],
   "source": [
    "## Global variables\n",
    "\n",
    "# Terra workspace related variables\n",
    "target_dir = \"fragpipe\"\n",
    "bucket = os.environ.get (\"WORKSPACE_BUCKET\")\n",
    "target_path = os.path.join (bucket, target_dir)\n",
    "\n",
    "# fragpipe related variables\n",
    "annot_file = \"annotation.txt\"\n",
    "manifest_file = \"fragpipe-manifest.fp-manifest\"\n",
    "manifest_df = pd.DataFrame(columns = ['Path', 'Experiment', 'Bioreplicate', 'DataType'])\n",
    "\n",
    "# URL for PDC API calls\n",
    "pdc_url = 'https://pdc.cancer.gov/graphql'\n",
    "\n",
    "# query for experimental design data from PDC\n",
    "query = '''{\n",
    "  studyExperimentalDesign(pdc_study_id: \"%s\" acceptDUA: true) {\n",
    "    acquisition_type\n",
    "    analyte\n",
    "    experiment_number\n",
    "    experiment_type\n",
    "    number_of_fractions\n",
    "    pdc_study_id\n",
    "    plex_dataset_name\n",
    "    study_id\n",
    "    study_run_metadata_id\n",
    "    study_run_metadata_submitter_id\n",
    "    study_submitter_id\n",
    "    label_free{aliquot_submitter_id}\n",
    "    itraq_113{aliquot_submitter_id}\n",
    "    itraq_114{aliquot_submitter_id}\n",
    "    itraq_115{aliquot_submitter_id}\n",
    "    itraq_116{aliquot_submitter_id}\n",
    "    itraq_117{aliquot_submitter_id} \n",
    "    itraq_118{aliquot_submitter_id}\n",
    "    itraq_119{aliquot_submitter_id}\n",
    "    itraq_121{aliquot_submitter_id}\n",
    "    tmt_126{aliquot_submitter_id}\n",
    "    tmt_127n{aliquot_submitter_id} \n",
    "    tmt_127c{aliquot_submitter_id}\n",
    "    tmt_128n{aliquot_submitter_id}\n",
    "    tmt_128c{aliquot_submitter_id}\n",
    "    tmt_129n{aliquot_submitter_id}\n",
    "    tmt_129c{aliquot_submitter_id} \n",
    "    tmt_130n{aliquot_submitter_id} \n",
    "    tmt_130c{aliquot_submitter_id}\n",
    "    tmt_131{aliquot_submitter_id} \n",
    "    tmt_131c{aliquot_submitter_id} \n",
    "    tmt_132n{aliquot_submitter_id}\n",
    "    tmt_132c{aliquot_submitter_id} \n",
    "    tmt_133n{aliquot_submitter_id}\n",
    "    tmt_133c{aliquot_submitter_id} \n",
    "    tmt_134n{aliquot_submitter_id} \n",
    "    tmt_134c{aliquot_submitter_id}\n",
    "    tmt_135n{aliquot_submitter_id}\n",
    "  }\n",
    "}'''\n",
    "\n",
    "# variables for managing input table, experiment design and annotations\n",
    "file_rows = table.list_rows (\"file\")\n",
    "expt_design = {}\n",
    "plex_annot = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Define functions to streamline code in the later blocks and to illustrate the flow of the script in a more conceptual manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     11,
     23,
     28,
     32,
     42,
     65
    ]
   },
   "outputs": [],
   "source": [
    "## Functions to streamline code\n",
    "\n",
    "def md5 (fname):\n",
    "  # generate MD5 checksum for given file\n",
    "  # from https://stackoverflow.com/questions/3431825/generating-an-md5-checksum-of-a-file\n",
    "  hash_md5 = hashlib.md5()\n",
    "  with open(fname, \"rb\") as f:\n",
    "    for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "      hash_md5.update(chunk)\n",
    "  return hash_md5.hexdigest()\n",
    "\n",
    "def download_file (url, f, checksum):\n",
    "  ret_file = f\n",
    "  # download file and check MD5 checksum\n",
    "  if os.path.exists (f):\n",
    "    os.remove (f)     # remove previous files, if any\n",
    "  drs.copy (url, f)\n",
    "  if (md5(f) != checksum):\n",
    "    sys.exit ('** ERROR: File download failed.')\n",
    "  # unzip file it is zipped (specifically for mzML.gz files)\n",
    "  fname, ext = os.path.splitext (f)\n",
    "  if ext == \".gz\":\n",
    "    ret_file = fname  # return file name without .gz extension\n",
    "    with gzip.open(f, 'rb') as f_in, open(fname, 'wb') as f_out:\n",
    "      shutil.copyfileobj(f_in, f_out)\n",
    "    \n",
    "  return ret_file\n",
    "  \n",
    "def transfer_to_bucket (subdir, f, root=target_path):\n",
    "  # transfer given file to specified subdirectory in the workspace bucket\n",
    "  os.system (\"gsutil -q cp \" + f + \" \" + os.path.join (root, subdir, f))\n",
    "\n",
    "def download_expt_design (study):\n",
    "  response = requests.post(pdc_url, json={'query': query % study})\n",
    "  if(response.ok):\n",
    "    jData = json.loads(response.content)\n",
    "    study_design = jData['data']['studyExperimentalDesign']\n",
    "    expt_design[study] = study_design\n",
    "  else:   # If response code is not ok (200)\n",
    "    response.raise_for_status()\n",
    "  return study_design\n",
    "\n",
    "def create_annotation (des, plx, subdir, a_file):\n",
    "  print ('** Writing annotation file ' + os.path.join (subdir, a_file))\n",
    "  if os.path.exists (a_file):\n",
    "    os.remove (a_file)    # remove previous files, if any\n",
    "  expt_type = des[plx]['experiment_type']\n",
    "  if (expt_type.startswith ('TMT') or expt_type.startswith ('iTRAQ')) and des[plx]['acquisition_type'] == \"DDA\": \n",
    "    # for TMT/iTRAQ experiments\n",
    "    # get TMT aliquot IDs and create annotation file\n",
    "    aliquots = {key: val[0]['aliquot_submitter_id'] \n",
    "                for key, val in des[plx].items() \n",
    "                  if (key.startswith('tmt') or key.startswith('itraq')) and val != None}\n",
    "    with open (a_file, \"a\") as f:\n",
    "      for i, k in enumerate (aliquots.keys()):\n",
    "        aliquot = aliquots[k].replace (\" \", \"\")\n",
    "        if 'Pool' in aliquot or 'pool' in aliquot:\n",
    "          aliquot = 'pool' + str (plx)\n",
    "        ch = k.split ('tmt_')[1].upper()\n",
    "        print (ch, aliquot, file=f)\n",
    "    transfer_to_bucket (subdir, a_file)\n",
    "  else:\n",
    "    # To-Do: Implement options for label-free, DIA\n",
    "    sys.exit ('** ERROR: Only TMT/iTRAQ DDA experiments supported at this time.')\n",
    "\n",
    "def write_manifest (df, m_file):\n",
    "  print ('** Writing manifest file ' + m_file)\n",
    "  if os.path.exists (m_file):\n",
    "    os.remove (m_file)    # remove previous files, if any\n",
    "  df.sort_values (by=['Experiment']).to_csv (m_file, header=None, index=None, sep='\\t', mode='a')\n",
    "  transfer_to_bucket ('', m_file, root=bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer the files\n",
    "Direct copy to the bucket using `drs.copy_to_bucket` does not work and results in an error -- download file to local disk and use `gsutil` to copy to the workspace bucket. As files are downloaded, use the PDC `graphql` API to get experimental design to create the annotation files. Also assemble and output the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/02CPTAC_LUAD_W_BI_20180518_KR_f02.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fed0e933b14caba0e8f3c9936a9a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=196913698), Label(value='187.8MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Writing annotation file S046-1-2/annotation.txt\n",
      "/home/jupyter/PDC-PFB-Test/edit/01CPTAC_LUAD_W_BI_20180515_KR_f04.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "688def70ee2347e4953d0812de2f0b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=212273950), Label(value='202.4MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Writing annotation file S046-1-1/annotation.txt\n",
      "/home/jupyter/PDC-PFB-Test/edit/02CPTAC_LUAD_W_BI_20180518_KR_f01.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922cb8f1ac5d46e79f0ddb8b268c32b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=181089928), Label(value='172.7MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/01CPTAC_LUAD_W_BI_20180515_KR_f03.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5b7be8775b4b8aa072fee9128583f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=189883393), Label(value='181.1MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/01CPTAC_LUAD_W_BI_20180515_KR_f02.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6304d4d3fb3b40fcaef2493900d19250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=184457405), Label(value='175.9MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/01CPTAC_LUAD_W_BI_20180515_KR_f05.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec258c135414af28a2e184e9e0f997c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=197325411), Label(value='188.2MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/02CPTAC_LUAD_W_BI_20180518_KR_f05.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4edc9bbb664fc1ac4abdd1bdf780fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=218764482), Label(value='208.6MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/02CPTAC_LUAD_W_BI_20180518_KR_f03.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b00338131794ac3aa55b97879005a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=195631676), Label(value='186.6MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/01CPTAC_LUAD_W_BI_20180515_KR_f01.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac3137a04d041199fdb63c7fae13fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=176299411), Label(value='168.1MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/PDC-PFB-Test/edit/02CPTAC_LUAD_W_BI_20180518_KR_f04.mzML.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fb2a5499fb43f391221f61c34f0f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value=''), IntProgress(value=0, max=212826145), Label(value='203.0MiB'), Label(value=''),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Writing manifest file fragpipe-manifest.fp-manifest\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Downloading the DRS_URL file may require the user to login to NCI CRDC Framework Services\n",
    "#       Set up access (if needed) using the EXTERNAL INDENTITIES tab in the USER PROFILE on Terra \n",
    "#         before running this code\n",
    "for row in file_rows:\n",
    "  # extract relevant fields from row\n",
    "  drs_url = row.attributes [\"pfb:object_id\"]\n",
    "  file_name = row.attributes [\"pfb:file_name\"]\n",
    "  expt_subdir = row.attributes [\"pfb:study_run_metadata_submitter_id\"]\n",
    "  pdc_study_id = row.attributes [\"pfb:pdc_study_id\"]\n",
    "  md5_checksum = row.attributes [\"pfb:file_md5sum\"]\n",
    "\n",
    "  # download file and transfer to workspace bucket\n",
    "  file_name = download_file (drs_url, file_name, md5_checksum)\n",
    "  transfer_to_bucket (expt_subdir, file_name)\n",
    "  os.remove (file_name)  # delete local file\n",
    "    \n",
    "  # add to manifest table\n",
    "  manifest_df = manifest_df.append ({'Path': os.path.join (\"/root/fragpipe/data\", expt_subdir, file_name),\n",
    "                                     'Experiment': expt_subdir, 'Bioreplicate': '', 'DataType': 'DDA'},\n",
    "                                    ignore_index=True)\n",
    "  # download experiment design metadata (if needed -- keep previous downloads)\n",
    "  if pdc_study_id in expt_design.keys():\n",
    "    design = expt_design[pdc_study_id]\n",
    "  else:\n",
    "    design = download_expt_design (pdc_study_id)\n",
    "\n",
    "  # get plex details and create required annotation files \n",
    "  plex = -1\n",
    "  for i in range (len (design)):\n",
    "    if design[i]['study_run_metadata_submitter_id'] == expt_subdir:\n",
    "      plex = i\n",
    "      break\n",
    "\n",
    "  if (plex_annot.get(plex) == None): \n",
    "    # current plex has not already been annotated\n",
    "    plex_annot[plex] = True\n",
    "    create_annotation (design, plex, expt_subdir, annot_file)\n",
    "\n",
    "# write out manifest file\n",
    "write_manifest (manifest_df, manifest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "281.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
